When reading from large files through the generic file read functions into
page cache we can detect when a file is so large that it is unlikely to be
fully cached in ram. If that happens we can put it on the tail end of the
inactive lru list so it can be the first thing evicted next time we need ram.

Do lots of funny buggers with underscores to preserve most of the existing
APIs.

-ck

---
 include/linux/mm_inline.h |   15 ++++++++++++---
 include/linux/pagemap.h   |    2 ++
 include/linux/swap.h      |    8 +++++++-
 mm/filemap.c              |   12 +++++++++---
 mm/readahead.c            |   32 ++++++++++++++++++++++++++++----
 mm/swap.c                 |   30 ++++++++++++++++++++++++------
 6 files changed, 82 insertions(+), 17 deletions(-)

Index: linux-3.0.0-ck1/include/linux/mm_inline.h
===================================================================
--- linux-3.0.0-ck1.orig/include/linux/mm_inline.h	2011-08-11 12:41:05.980183573 +1000
+++ linux-3.0.0-ck1/include/linux/mm_inline.h	2011-08-11 12:44:12.241183568 +1000
@@ -23,9 +23,12 @@ static inline int page_is_file_cache(str
 
 static inline void
 __add_page_to_lru_list(struct zone *zone, struct page *page, enum lru_list l,
-		       struct list_head *head)
+		       struct list_head *head, int tail)
 {
-	list_add(&page->lru, head);
+	if (tail)
+		list_add_tail(&page->lru, head);
+	else
+		list_add(&page->lru, head);
 	__mod_zone_page_state(zone, NR_LRU_BASE + l, hpage_nr_pages(page));
 	mem_cgroup_add_lru_list(page, l);
 }
@@ -33,7 +36,13 @@ __add_page_to_lru_list(struct zone *zone
 static inline void
 add_page_to_lru_list(struct zone *zone, struct page *page, enum lru_list l)
 {
-	__add_page_to_lru_list(zone, page, l, &zone->lru[l].list);
+	__add_page_to_lru_list(zone, page, l, &zone->lru[l].list, 0);
+}
+
+static inline void
+add_page_to_lru_list_tail(struct zone *zone, struct page *page, enum lru_list l)
+{
+	__add_page_to_lru_list(zone, page, l, &zone->lru[l].list, 1);
 }
 
 static inline void
Index: linux-3.0.0-ck1/include/linux/swap.h
===================================================================
--- linux-3.0.0-ck1.orig/include/linux/swap.h	2011-08-11 12:44:11.225183568 +1000
+++ linux-3.0.0-ck1/include/linux/swap.h	2011-08-11 12:44:12.241183568 +1000
@@ -215,6 +215,7 @@ extern unsigned int nr_free_pagecache_pa
 
 
 /* linux/mm/swap.c */
+extern void ____lru_cache_add(struct page *, enum lru_list lru, int tail);
 extern void __lru_cache_add(struct page *, enum lru_list lru);
 extern void lru_cache_add_lru(struct page *, enum lru_list lru);
 extern void lru_add_page_tail(struct zone* zone,
@@ -238,9 +239,14 @@ static inline void lru_cache_add_anon(st
 	__lru_cache_add(page, LRU_INACTIVE_ANON);
 }
 
+static inline void lru_cache_add_file_tail(struct page *page, int tail)
+{
+	____lru_cache_add(page, LRU_INACTIVE_FILE, tail);
+}
+
 static inline void lru_cache_add_file(struct page *page)
 {
-	__lru_cache_add(page, LRU_INACTIVE_FILE);
+	____lru_cache_add(page, LRU_INACTIVE_FILE, 0);
 }
 
 /* LRU Isolation modes. */
Index: linux-3.0.0-ck1/mm/filemap.c
===================================================================
--- linux-3.0.0-ck1.orig/mm/filemap.c	2011-08-11 12:41:06.024183573 +1000
+++ linux-3.0.0-ck1/mm/filemap.c	2011-08-11 12:44:12.242183568 +1000
@@ -498,8 +498,8 @@ out:
 }
 EXPORT_SYMBOL(add_to_page_cache_locked);
 
-int add_to_page_cache_lru(struct page *page, struct address_space *mapping,
-				pgoff_t offset, gfp_t gfp_mask)
+int __add_to_page_cache_lru(struct page *page, struct address_space *mapping,
+				pgoff_t offset, gfp_t gfp_mask, int tail)
 {
 	int ret;
 
@@ -515,12 +515,18 @@ int add_to_page_cache_lru(struct page *p
 	ret = add_to_page_cache(page, mapping, offset, gfp_mask);
 	if (ret == 0) {
 		if (page_is_file_cache(page))
-			lru_cache_add_file(page);
+			lru_cache_add_file_tail(page, tail);
 		else
 			lru_cache_add_anon(page);
 	}
 	return ret;
 }
+
+int add_to_page_cache_lru(struct page *page, struct address_space *mapping,
+				pgoff_t offset, gfp_t gfp_mask)
+{
+	return __add_to_page_cache_lru(page, mapping, offset, gfp_mask, 0);
+}
 EXPORT_SYMBOL_GPL(add_to_page_cache_lru);
 
 #ifdef CONFIG_NUMA
Index: linux-3.0.0-ck1/mm/swap.c
===================================================================
--- linux-3.0.0-ck1.orig/mm/swap.c	2011-08-11 12:41:06.013183573 +1000
+++ linux-3.0.0-ck1/mm/swap.c	2011-08-11 12:44:12.242183568 +1000
@@ -348,15 +348,23 @@ void mark_page_accessed(struct page *pag
 
 EXPORT_SYMBOL(mark_page_accessed);
 
-void __lru_cache_add(struct page *page, enum lru_list lru)
+void ______pagevec_lru_add(struct pagevec *pvec, enum lru_list lru, int tail);
+
+void ____lru_cache_add(struct page *page, enum lru_list lru, int tail)
 {
 	struct pagevec *pvec = &get_cpu_var(lru_add_pvecs)[lru];
 
 	page_cache_get(page);
 	if (!pagevec_add(pvec, page))
-		____pagevec_lru_add(pvec, lru);
+		______pagevec_lru_add(pvec, lru, tail);
 	put_cpu_var(lru_add_pvecs);
 }
+EXPORT_SYMBOL(____lru_cache_add);
+
+void __lru_cache_add(struct page *page, enum lru_list lru)
+{
+	____lru_cache_add(page, lru, 0);
+}
 EXPORT_SYMBOL(__lru_cache_add);
 
 /**
@@ -364,7 +372,7 @@ EXPORT_SYMBOL(__lru_cache_add);
  * @page: the page to be added to the LRU.
  * @lru: the LRU list to which the page is added.
  */
-void lru_cache_add_lru(struct page *page, enum lru_list lru)
+void __lru_cache_add_lru(struct page *page, enum lru_list lru, int tail)
 {
 	if (PageActive(page)) {
 		VM_BUG_ON(PageUnevictable(page));
@@ -375,7 +383,12 @@ void lru_cache_add_lru(struct page *page
 	}
 
 	VM_BUG_ON(PageLRU(page) || PageActive(page) || PageUnevictable(page));
-	__lru_cache_add(page, lru);
+	____lru_cache_add(page, lru, tail);
+}
+
+void lru_cache_add_lru(struct page *page, enum lru_list lru)
+{
+	__lru_cache_add_lru(page, lru, 0);
 }
 
 /**
@@ -662,7 +675,7 @@ void lru_add_page_tail(struct zone* zone
 			head = page->lru.prev;
 		else
 			head = &zone->lru[lru].list;
-		__add_page_to_lru_list(zone, page_tail, lru, head);
+		__add_page_to_lru_list(zone, page_tail, lru, head, 0);
 	} else {
 		SetPageUnevictable(page_tail);
 		add_page_to_lru_list(zone, page_tail, LRU_UNEVICTABLE);
@@ -691,13 +704,18 @@ static void ____pagevec_lru_add_fn(struc
  * Add the passed pages to the LRU, then drop the caller's refcount
  * on them.  Reinitialises the caller's pagevec.
  */
-void ____pagevec_lru_add(struct pagevec *pvec, enum lru_list lru)
+void ______pagevec_lru_add(struct pagevec *pvec, enum lru_list lru, int tail)
 {
 	VM_BUG_ON(is_unevictable_lru(lru));
 
 	pagevec_lru_move_fn(pvec, ____pagevec_lru_add_fn, (void *)lru);
 }
 
+void ____pagevec_lru_add(struct pagevec *pvec, enum lru_list lru)
+{
+	______pagevec_lru_add(pvec, lru, 0);
+}
+
 EXPORT_SYMBOL(____pagevec_lru_add);
 
 /*
Index: linux-3.0.0-ck1/mm/readahead.c
===================================================================
--- linux-3.0.0-ck1.orig/mm/readahead.c	2011-08-11 12:41:06.005183573 +1000
+++ linux-3.0.0-ck1/mm/readahead.c	2011-08-11 12:44:12.243183568 +1000
@@ -17,6 +17,7 @@
 #include <linux/task_io_accounting_ops.h>
 #include <linux/pagevec.h>
 #include <linux/pagemap.h>
+#include <linux/swap.h>
 
 /*
  * Initialise a struct file's readahead state.  Assumes that the caller has
@@ -107,7 +108,7 @@ int read_cache_pages(struct address_spac
 EXPORT_SYMBOL(read_cache_pages);
 
 static int read_pages(struct address_space *mapping, struct file *filp,
-		struct list_head *pages, unsigned nr_pages)
+		struct list_head *pages, unsigned nr_pages, int tail)
 {
 	struct blk_plug plug;
 	unsigned page_idx;
@@ -125,8 +126,8 @@ static int read_pages(struct address_spa
 	for (page_idx = 0; page_idx < nr_pages; page_idx++) {
 		struct page *page = list_to_page(pages);
 		list_del(&page->lru);
-		if (!add_to_page_cache_lru(page, mapping,
-					page->index, GFP_KERNEL)) {
+		if (!__add_to_page_cache_lru(page, mapping,
+					page->index, GFP_KERNEL, tail)) {
 			mapping->a_ops->readpage(filp, page);
 		}
 		page_cache_release(page);
@@ -139,6 +140,28 @@ out:
 	return ret;
 }
 
+static inline int nr_mapped(void)
+{
+	return global_page_state(NR_FILE_MAPPED) +
+		global_page_state(NR_ANON_PAGES);
+}
+
+/*
+ * This examines how large in pages a file size is and returns 1 if it is
+ * more than half the unmapped ram. Avoid doing read_page_state which is
+ * expensive unless we already know it is likely to be large enough.
+ */
+static int large_isize(unsigned long nr_pages)
+{
+	if (nr_pages * 6 > vm_total_pages) {
+		 unsigned long unmapped_ram = vm_total_pages - nr_mapped();
+
+		if (nr_pages * 2 > unmapped_ram)
+			return 1;
+	}
+	return 0;
+}
+
 /*
  * __do_page_cache_readahead() actually reads a chunk of disk.  It allocates all
  * the pages first, then submits them all for I/O. This avoids the very bad
@@ -196,7 +219,8 @@ __do_page_cache_readahead(struct address
 	 * will then handle the error.
 	 */
 	if (ret)
-		read_pages(mapping, filp, &page_pool, ret);
+		read_pages(mapping, filp, &page_pool, ret,
+			   large_isize(end_index));
 	BUG_ON(!list_empty(&page_pool));
 out:
 	return ret;
Index: linux-3.0.0-ck1/include/linux/pagemap.h
===================================================================
--- linux-3.0.0-ck1.orig/include/linux/pagemap.h	2011-08-11 12:41:05.998183573 +1000
+++ linux-3.0.0-ck1/include/linux/pagemap.h	2011-08-11 12:44:12.243183568 +1000
@@ -458,6 +458,8 @@ int add_to_page_cache_locked(struct page
 				pgoff_t index, gfp_t gfp_mask);
 int add_to_page_cache_lru(struct page *page, struct address_space *mapping,
 				pgoff_t index, gfp_t gfp_mask);
+int __add_to_page_cache_lru(struct page *page, struct address_space *mapping,
+				pgoff_t offset, gfp_t gfp_mask, int tail);
 extern void delete_from_page_cache(struct page *page);
 extern void __delete_from_page_cache(struct page *page);
 int replace_page_cache_page(struct page *old, struct page *new, gfp_t gfp_mask);
