Add an "above background load" function which can be used for background
tasks elsewhere (e.g. VM).

-ck
---
 include/linux/sched.h |    7 +++++++
 kernel/sched_bfs.c    |   20 ++++++++++++++++++++
 2 files changed, 27 insertions(+)

Index: linux-3.0.0-ck1/include/linux/sched.h
===================================================================
--- linux-3.0.0-ck1.orig/include/linux/sched.h	2011-08-11 12:41:08.451183572 +1000
+++ linux-3.0.0-ck1/include/linux/sched.h	2011-08-11 12:44:10.500183568 +1000
@@ -1619,6 +1619,7 @@ static inline int iso_task(struct task_s
 	return (p->policy == SCHED_ISO);
 }
 extern void remove_cpu(unsigned long cpu);
+extern int above_background_load(void);
 #else /* CFS */
 extern int runqueue_is_locked(int cpu);
 static inline void cpu_scaling(int cpu)
@@ -1649,6 +1650,12 @@ static inline int iso_task(struct task_s
 static inline void remove_cpu(unsigned long cpu)
 {
 }
+
+/* Anyone feel like implementing this? */
+static inline int above_background_load(void)
+{
+	return 1;
+}
 #endif /* CONFIG_SCHED_BFS */
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
Index: linux-3.0.0-ck1/kernel/sched_bfs.c
===================================================================
--- linux-3.0.0-ck1.orig/kernel/sched_bfs.c	2011-08-11 12:43:48.937183569 +1000
+++ linux-3.0.0-ck1/kernel/sched_bfs.c	2011-08-11 12:44:10.505183568 +1000
@@ -569,6 +569,26 @@ static inline void __task_grq_unlock(voi
 	grq_unlock();
 }
 
+/*
+ * Look for any tasks *anywhere* that are running nice 0 or better. We do
+ * this lockless for overhead reasons since the occasional wrong result
+ * is harmless.
+ */
+int above_background_load(void)
+{
+	struct task_struct *cpu_curr;
+	unsigned long cpu;
+
+	for_each_online_cpu(cpu) {
+		cpu_curr = cpu_rq(cpu)->curr;
+		if (unlikely(!cpu_curr))
+			continue;
+		if (PRIO_TO_NICE(cpu_curr->static_prio) < 1)
+			return 1;
+	}
+	return 0;
+}
+
 #ifndef __ARCH_WANT_UNLOCKED_CTXSW
 static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
 {
